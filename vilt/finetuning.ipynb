{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "\n",
    "# Load BLIP model and processor\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        image = Image.open(BytesIO(row['image']['bytes'])).convert(\"RGB\")\n",
    "        \n",
    "        encoding = self.processor(image, question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            answer, max_length=8, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        return encoding\n",
    "\n",
    "# Load your dataframe\n",
    "splits = {'train': 'data/train-00000-of-00001-eb8844602202be60.parquet', 'test': 'data/test-00000-of-00001-e5bc3d208bb4deeb.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/flaviagiammarino/vqa-rad/\" + splits[\"train\"])\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "valid_df = df.drop(train_df.index)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = VQADataset(dataframe=train_df, processor=processor)\n",
    "valid_dataset = VQADataset(dataframe=valid_df, processor=processor)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 12\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "min_eval_loss = float(\"inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader, desc=f\"Validating Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    train_loss = epoch_loss / len(train_dataloader)\n",
    "    valid_loss = eval_loss / len(valid_dataloader)\n",
    "    tracking_information.append((train_loss, valid_loss, optimizer.param_groups[0][\"lr\"]))\n",
    "    print(f\"Epoch: {epoch+1} - Training loss: {train_loss:.4f} - Validation loss: {valid_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.4e}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if valid_loss < min_eval_loss:\n",
    "        model.save_pretrained(\"Model/blip-saved-model\")\n",
    "        processor.save_pretrained(\"Model/blip-saved-model\")\n",
    "        print(\"Saved model to Model/blip-saved-model\")\n",
    "        min_eval_loss = valid_loss\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "pickle.dump(tracking_information, open(\"tracking_information.pkl\", \"wb\"))\n",
    "print(\"The finetuning process is complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageClef-2019-VQA-Med Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "\n",
    "# Load BLIP model and processor\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the dataset class\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, image_folder, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        \n",
    "        # Load image\n",
    "        image_path = os.path.join(self.image_folder, f\"{image_id}.jpg\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Process image and text\n",
    "        encoding = self.processor(image, question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            answer, max_length=8, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        return encoding\n",
    "\n",
    "# Load the question-answer pairs\n",
    "data_path = \"data/ImageClef-2019-VQA-Med/ImageClef-2019-VQA-Med-Training/All_QA_Pairs_train.txt\"\n",
    "image_folder = \"data/ImageClef-2019-VQA-Med/ImageClef-2019-VQA-Med-Training/Train_images\"\n",
    "\n",
    "# Read the data\n",
    "data = []\n",
    "with open(data_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('|')\n",
    "        if len(parts) == 3:\n",
    "            image_id, question, answer = parts\n",
    "            data.append({'image_id': image_id, 'question': question, 'answer': answer})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "valid_df = df.drop(train_df.index)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = VQADataset(dataframe=train_df, image_folder=image_folder, processor=processor)\n",
    "valid_dataset = VQADataset(dataframe=valid_df, image_folder=image_folder, processor=processor)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 12\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 4\n",
    "min_eval_loss = float(\"inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader, desc=f\"Validating Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels, attention_mask=attention_mask)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    train_loss = epoch_loss / len(train_dataloader)\n",
    "    valid_loss = eval_loss / len(valid_dataloader)\n",
    "    tracking_information.append((train_loss, valid_loss, optimizer.param_groups[0][\"lr\"]))\n",
    "    print(f\"Epoch: {epoch+1} - Training loss: {train_loss:.4f} - Validation loss: {valid_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.4e}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if valid_loss < min_eval_loss:\n",
    "        model.save_pretrained(\"Model_2/blip-saved-model\")\n",
    "        processor.save_pretrained(\"Model_2/blip-saved-model\")\n",
    "        print(\"Saved model to Model_2/blip-saved-model\")\n",
    "        min_eval_loss = valid_loss\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "pickle.dump(tracking_information, open(\"tracking_information.pkl\", \"wb\"))\n",
    "print(\"The finetuning process is complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "\n",
    "# Load BLIP model and processor\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Model/blip-saved-model\")\n",
    "processor = BlipProcessor.from_pretrained(\"Model/blip-saved-model\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the dataset class\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, image_folder, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        \n",
    "        # Load image\n",
    "        image_path = os.path.join(self.image_folder, f\"{image_id}.jpg\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Process image and text\n",
    "        encoding = self.processor(image, question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            answer, max_length=8, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        return encoding\n",
    "\n",
    "# Load the question-answer pairs\n",
    "data_path = \"data/ImageClef-2019-VQA-Med/ImageClef-2019-VQA-Med-Training/QAPairsByCategory/C2_Plane_train.txt\"\n",
    "image_folder = \"data/ImageClef-2019-VQA-Med/ImageClef-2019-VQA-Med-Training/Train_images\"\n",
    "\n",
    "# Read the data\n",
    "data = []\n",
    "with open(data_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('|')\n",
    "        if len(parts) == 3:\n",
    "            image_id, question, answer = parts\n",
    "            data.append({'image_id': image_id, 'question': question, 'answer': answer})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "valid_df = df.drop(train_df.index)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = VQADataset(dataframe=train_df, image_folder=image_folder, processor=processor)\n",
    "valid_dataset = VQADataset(dataframe=valid_df, image_folder=image_folder, processor=processor)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 12\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 3\n",
    "min_eval_loss = float(\"inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader, desc=f\"Validating Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels, attention_mask=attention_mask)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    train_loss = epoch_loss / len(train_dataloader)\n",
    "    valid_loss = eval_loss / len(valid_dataloader)\n",
    "    tracking_information.append((train_loss, valid_loss, optimizer.param_groups[0][\"lr\"]))\n",
    "    print(f\"Epoch: {epoch+1} - Training loss: {train_loss:.4f} - Validation loss: {valid_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.4e}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if valid_loss < min_eval_loss:\n",
    "        model.save_pretrained(\"Model/blip-saved-model\")\n",
    "        processor.save_pretrained(\"Model/blip-saved-model\")\n",
    "        print(\"Saved model to Model/blip-saved-model\")\n",
    "        min_eval_loss = valid_loss\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "pickle.dump(tracking_information, open(\"tracking_information_1.pkl\", \"wb\"))\n",
    "print(\"The finetuning process is complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "\n",
    "# Load BLIP model and processor\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Model/blip-saved-model\")\n",
    "processor = BlipProcessor.from_pretrained(\"Model/blip-saved-model\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the dataset class\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, image_folder, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        \n",
    "        # Load image\n",
    "        image_path = os.path.join(self.image_folder, f\"{image_id}.jpg\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Process image and text\n",
    "        encoding = self.processor(image, question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            answer, max_length=8, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        return encoding\n",
    "\n",
    "# Load the question-answer pairs\n",
    "data_path = \"data/ImageClef-2019-VQA-Med/ImageClef-2019-VQA-Med-Training/QAPairsByCategory/C3_Organ_train.txt\"\n",
    "image_folder = \"data/ImageClef-2019-VQA-Med/ImageClef-2019-VQA-Med-Training/Train_images\"\n",
    "\n",
    "# Read the data\n",
    "data = []\n",
    "with open(data_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('|')\n",
    "        if len(parts) == 3:\n",
    "            image_id, question, answer = parts\n",
    "            data.append({'image_id': image_id, 'question': question, 'answer': answer})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "valid_df = df.drop(train_df.index)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = VQADataset(dataframe=train_df, image_folder=image_folder, processor=processor)\n",
    "valid_dataset = VQADataset(dataframe=valid_df, image_folder=image_folder, processor=processor)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 12\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 3\n",
    "min_eval_loss = float(\"inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "loss_val = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        loss_val.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader, desc=f\"Validating Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels, attention_mask=attention_mask)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    train_loss = epoch_loss / len(train_dataloader)\n",
    "    valid_loss = eval_loss / len(valid_dataloader)\n",
    "    tracking_information.append((train_loss, valid_loss, optimizer.param_groups[0][\"lr\"]))\n",
    "    print(f\"Epoch: {epoch+1} - Training loss: {train_loss:.4f} - Validation loss: {valid_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.4e}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if valid_loss < min_eval_loss:\n",
    "        model.save_pretrained(\"Model/blip-saved-model\")\n",
    "        processor.save_pretrained(\"Model/blip-saved-model\")\n",
    "        print(\"Saved model to Model/blip-saved-model\")\n",
    "        min_eval_loss = valid_loss\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "pickle.dump(tracking_information, open(\"tracking_information_2.pkl\", \"wb\"))\n",
    "print(\"The finetuning process is complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# write loss_list to a file\n",
    "with open('loss_list_blip2.csv', 'w') as f:\n",
    "    for item in loss_val:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "# plot loss_list\n",
    "plt.figure(figsize=(12, 6)) \n",
    "step = 2 \n",
    "loss_epoch = loss_val[240:481]\n",
    "plt.plot(loss_epoch[::step], color='blue', linewidth=1.0)  \n",
    "plt.title(\"Epoch 1\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "\n",
    "# Load BLIP model and processor\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Model/blip-saved-model\")\n",
    "processor = BlipProcessor.from_pretrained(\"Model/blip-saved-model\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the dataset class\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, image_folder, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        \n",
    "        # Load image\n",
    "        image_path = os.path.join(self.image_folder, f\"{image_id}.jpg\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Process image and text\n",
    "        encoding = self.processor(image, question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "            answer, max_length=8, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        return encoding\n",
    "\n",
    "# Load the question-answer pairs\n",
    "data_path = \"data/ImageClef-2019-VQA-Med/ImageClef-2019-VQA-Med-Training/QAPairsByCategory/C4_Abnormality_train.txt\"\n",
    "image_folder = \"data/ImageClef-2019-VQA-Med/ImageClef-2019-VQA-Med-Training/Train_images\"\n",
    "\n",
    "# Read the data\n",
    "data = []\n",
    "with open(data_path, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split('|')\n",
    "        if len(parts) == 3:\n",
    "            image_id, question, answer = parts\n",
    "            data.append({'image_id': image_id, 'question': question, 'answer': answer})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_df = df.sample(frac=0.9, random_state=42)\n",
    "valid_df = df.drop(train_df.index)\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = VQADataset(dataframe=train_df, image_folder=image_folder, processor=processor)\n",
    "valid_dataset = VQADataset(dataframe=valid_df, image_folder=image_folder, processor=processor)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 12\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP2 - 227k Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datasets import DatasetDict, load_from_disk\n",
    "import os\n",
    "from transformers import ViltConfig\n",
    "import torch\n",
    "import io\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data set combined_datasets\n",
    "train= load_from_disk('./PreprocessedData/train')\n",
    "validation= load_from_disk('./PreprocessedData/validation')\n",
    "\n",
    "dataset_dict= DatasetDict({'train': train, 'validation': validation})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure image paths are processed correctly\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        image_path = data['image_path'].replace('\\\\', '/')\n",
    "        question = data['question']\n",
    "        answer = data['answer']\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Use the BLIP2 processor to process image and question\n",
    "        encoding = self.processor(image, question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "        answer, max_length=8, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        return encoding\n",
    "    \n",
    "# Load datasets\n",
    "train_dataset = VQADataset(dataset=dataset_dict['train'], processor=processor)\n",
    "validation_dataset = VQADataset(dataset=dataset_dict['validation'], processor=processor)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load BLIP2 processor and model\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16).to('cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "min_eval_loss = float(\"inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        #scaler.step(optimizer)\n",
    "        #scaler.update()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader, desc=f\"Validating Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    train_loss = epoch_loss / len(train_dataloader)\n",
    "    valid_loss = eval_loss / len(valid_dataloader)\n",
    "    tracking_information.append((train_loss, valid_loss, optimizer.param_groups[0][\"lr\"]))\n",
    "    print(f\"Epoch: {epoch+1} - Training loss: {train_loss:.4f} - Validation loss: {valid_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.4e}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if valid_loss < min_eval_loss:\n",
    "        model.save_pretrained(\"Model/blip-saved-model\")\n",
    "        processor.save_pretrained(\"Model/blip-saved-model\")\n",
    "        print(\"Saved model to Model/blip-saved-model\")\n",
    "        min_eval_loss = valid_loss\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "pickle.dump(tracking_information, open(\"tracking_information.pkl\", \"wb\"))\n",
    "print(\"The finetuning process is complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "import requests\n",
    "from PIL import Image\n",
    "import json, os, csv\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Set the path to your test data directory\n",
    "test_data_dir = \"Data/test_data/test_data\"\n",
    "\n",
    "# processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "# model = ViltForQuestionAnswering.from_pretrained(\"test_model/checkpoint-525\")\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Model/blip-saved-model\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Model/blip-saved-model\").to(\"cuda\")\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate through each file in the test data directory\n",
    "samples = os.listdir(test_data_dir)\n",
    "for filename in tqdm(os.listdir(test_data_dir), desc=\"Processing\"):\n",
    "    sample_path = f\"Data/test_data/{filename}\"\n",
    "\n",
    "    # Read the json file\n",
    "    json_path = os.path.join(sample_path, \"data.json\")\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        question = data[\"question\"]\n",
    "        image_id = data[\"id\"]\n",
    "\n",
    "    # Read the corresponding image\n",
    "    image_path = os.path.join(test_data_dir, f\"{image_id}\", \"image.png\")\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # prepare inputs\n",
    "    encoding = processor(image, question, return_tensors=\"pt\").to(\"cuda:0\", torch.float16)\n",
    "\n",
    "    out = model.generate(**encoding)\n",
    "    generated_text = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    results.append((image_id, generated_text))\n",
    "\n",
    "# Write the results to a CSV file\n",
    "csv_file_path = \"Results/results.csv\"\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"ID\", \"Label\"])  # Write header\n",
    "    csv_writer.writerows(results)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Model_2/blip-saved-model\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Model_2/blip-saved-model\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = 'https://prod-images-static.radiopaedia.org/images/17054297/07b3ca19d485b21a30bd8412dbbc33_big_gallery.jpeg'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw)\n",
    "raw_image.show()\n",
    "\n",
    "\n",
    "txt = \"is this person sick?\"\n",
    "question = f\"Question: {txt} Answer:\"\n",
    "inputs = processor(images=raw_image, text=question, return_tensors=\"pt\").to(device=\"cuda\")\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(raw_image)\n",
    "plt.axis('off') \n",
    "plt.show()\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_length=512)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
