{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_from_disk\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from transformers import ViltConfig\n",
    "import torch\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ViltConfig.from_pretrained(\"vilt-Med_PMC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data set combined_datasets\n",
    "train= load_from_disk('./PreprocessedData/train')\n",
    "validation= load_from_disk('./PreprocessedData/validation')\n",
    "\n",
    "dataset_dict= DatasetDict({'train': train, 'validation': validation})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"VQA dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, processor, id2label):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.id2label = id2label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image, question, and label\n",
    "        data = self.dataset[idx]\n",
    "        image_path = data['image_path'].replace('\\\\', '/')\n",
    "        question = data['question']\n",
    "        label = data['label']\n",
    "\n",
    "        # Open image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        #image = data['image']\n",
    "        \n",
    "        # Process image and question\n",
    "        encoding = self.processor(image, question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "\n",
    "        # Create target tensor\n",
    "        targets = torch.zeros(len(self.id2label))\n",
    "        targets[label] = 1  # Set the label index to 1\n",
    "\n",
    "        encoding[\"labels\"] = targets\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import ViltProcessor\n",
    "\n",
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "\n",
    "train_dataset = VQADataset(dataset=dataset_dict['train'], processor=processor, id2label=config.id2label)\n",
    "validation_dataset = VQADataset(dataset=dataset_dict['validation'], processor=processor, id2label=config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.decode(train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.nonzero(train_dataset[0]['labels']).squeeze().tolist()\n",
    "\n",
    "config.id2label[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltForQuestionAnswering\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"./vilt-PMC_VQA\",\n",
    "                                                 id2label=config.id2label,\n",
    "                                                 label2id=config.label2id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltForQuestionAnswering\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-mlm\",\n",
    "                                                 id2label=config.id2label,\n",
    "                                                 label2id=config.label2id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "  input_ids = [item['input_ids'] for item in batch]\n",
    "  pixel_values = [item['pixel_values'] for item in batch]\n",
    "  attention_mask = [item['attention_mask'] for item in batch]\n",
    "  token_type_ids = [item['token_type_ids'] for item in batch]\n",
    "  labels = [item['labels'] for item in batch]\n",
    "\n",
    "  # create padded pixel values and corresponding pixel mask\n",
    "  encoding = processor.image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "\n",
    "  # create new batch\n",
    "  batch = {}\n",
    "  batch['input_ids'] = torch.stack(input_ids)\n",
    "  batch['attention_mask'] = torch.stack(attention_mask)\n",
    "  batch['token_type_ids'] = torch.stack(token_type_ids)\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['labels'] = torch.stack(labels)\n",
    "\n",
    "  return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, collate_fn=collate_fn, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader), len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "image_mean = processor.image_processor.image_mean\n",
    "image_std = processor.image_processor.image_std\n",
    "\n",
    "batch_idx = 0\n",
    "\n",
    "unnormalized_image = (batch[\"pixel_values\"][batch_idx].numpy() * np.array(image_mean)[:, None, None]) + np.array(image_std)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "Image.fromarray(unnormalized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.decode(batch[\"input_ids\"][batch_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.nonzero(batch['labels'][batch_idx]).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.id2label[labels] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the directory to save checkpoints\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define the save step (e.g., save every 200 steps)\n",
    "save_steps = 10000\n",
    "global_step = 0\n",
    "\n",
    "# Define early stopping criteria\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Define the function to save the checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, step, checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint-epoch-{epoch}-step-{step}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "# Function to load the checkpoint\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        step = checkpoint['step']\n",
    "        print(f\"Resumed from epoch {epoch}, step {step}\")\n",
    "        return epoch, step\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        return 0, 0  # Starting from scratch if no checkpoint\n",
    "\n",
    "\n",
    "# Function to evaluate the model on the validation set\n",
    "def evaluate(model, validation_dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(validation_dataloader)\n",
    "    model.train()  # Set the model back to training mode\n",
    "    return avg_loss\n",
    "\n",
    "# Training loop\n",
    "loss_list = []\n",
    "model.train()\n",
    "\n",
    "# Modify the training loop to start from the correct epoch and step\n",
    "for epoch in range(1, 50):  # Start from the loaded epoch\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Training logic (same as before)\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Increment global step\n",
    "        global_step += 1\n",
    "\n",
    "        # Save the model at save_steps intervals\n",
    "        if global_step % save_steps == 0:\n",
    "            # Save latest checkpoint\n",
    "            save_checkpoint(model, optimizer, epoch, global_step, checkpoint_dir)\n",
    "            #model.save_pretrained(\"./vilt-PMC_VQA\")\n",
    "\n",
    "    # Evaluate and save at the end of each epoch\n",
    "    val_loss = evaluate(model, validation_dataloader)\n",
    "    print(f\"Validation Loss: {val_loss}\")\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        # Save the best model as the latest checkpoint\n",
    "        save_checkpoint(model, optimizer, epoch, global_step, checkpoint_dir)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# write loss_list to a file\n",
    "with open('loss_list.csv', 'w') as f:\n",
    "    for item in loss_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "# plot loss_list\n",
    "plt.figure(figsize=(12, 6)) \n",
    "step = 106139 // 100 #calculate the step size based on each epoch's size\n",
    "loss_epoch = loss_list[0:106139]\n",
    "plt.plot(loss_epoch[::step], color='blue', linewidth=1.0)  \n",
    "plt.title(\"Epoch 0 (downsampled)\", fontsize=16)  \n",
    "plt.xlabel(\"Steps\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.7)\n",
    "plt.xticks(fontsize=12) \n",
    "plt.yticks(fontsize=12) \n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in local directory\n",
    "model.save_pretrained(\"./vilt-PMC_VQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on sample picked validation dataset\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# Pick a random sample from the validation dataset\n",
    "sample = random.choice(validation_dataset)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the sample\n",
    "sample = {k: v.unsqueeze(0).to(device) for k, v in sample.items()}\n",
    "outputs = model(**sample)\n",
    "\n",
    "# Get the predicted label\n",
    "predicted_label = outputs.logits.argmax().item()\n",
    "predicted_answer = config.id2label[predicted_label]\n",
    "predicted_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ground truth question image und answer\n",
    "question = processor.decode(sample['input_ids'].squeeze())\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = config.id2label[torch.nonzero(sample['labels'].squeeze()).item()]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_image = (sample['pixel_values'].squeeze().cpu().numpy() * np.array(image_mean)[:, None, None]) + np.array(image_std)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "Image.fromarray(unnormalized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the directory to save checkpoints\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define the save step (e.g., save every 200 steps)\n",
    "save_steps = 200\n",
    "global_step = 0\n",
    "\n",
    "# Define the function to save the checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, step, checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint-epoch-{epoch}-step-{step}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs;\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        print(\"Loss:\", loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Increment global step\n",
    "        global_step += 1\n",
    "\n",
    "        # Save the model at save_steps intervals\n",
    "        if global_step % save_steps == 0:\n",
    "            save_checkpoint(model, optimizer, epoch, global_step, checkpoint_dir)\n",
    "\n",
    "    # Save the model at the end of each epoch\n",
    "    save_checkpoint(model, optimizer, epoch, global_step, checkpoint_dir)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from tqdm.notebook import tqdm\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "      print(f\"Epoch: {epoch}\")\n",
    "      for batch in tqdm(train_dataloader):\n",
    "            # get the inputs;\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            print(\"Loss:\", loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
