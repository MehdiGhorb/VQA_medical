{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2ForConditionalGeneration, Blip2Processor, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets, load_from_disk\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map=\"auto\")\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on {device}\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, \"../blip2/Model_2/blip-saved-model\")\n",
    "peft_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, load_from_disk\n",
    "# load data set combined_datasets\n",
    "train= load_from_disk('./PreprocessedData/train')\n",
    "validation= load_from_disk('./PreprocessedData/validation')\n",
    "\n",
    "dataset_dict= DatasetDict({'train': train, 'validation': validation})\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    pixel_values = [item['pixel_values'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad the sequences\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    # Stack the pixel values\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "# Define the dataset class\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        image_path = data['image_path'].replace('\\\\', '/')\n",
    "        question = data['question']\n",
    "        answer = data['answer']\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Use the BLIP2 processor to process image and question\n",
    "        encoding = self.processor(image, question, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        labels = self.processor.tokenizer.encode(\n",
    "        answer, pad_to_max_length=True, return_tensors='pt'\n",
    "        )\n",
    "        encoding[\"labels\"] = labels\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        return encoding\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = VQADataset(dataset=dataset_dict['train'], processor=processor)\n",
    "validation_dataset = VQADataset(dataset=dataset_dict['validation'], processor=processor)\n",
    "\n",
    "# Create DataLoader\n",
    "#train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, collate_fn=collate_fn)\n",
    "#valid_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_dict['validation'][100]\n",
    "\n",
    "image_path = data['image_path'].replace('\\\\', '/')\n",
    "question = data['question'] + \"Answer: \"\n",
    "true_answer = data['answer']\n",
    "\n",
    "# Load and process the image and question\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "inputs = processor(image, question, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate prediction\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=50)\n",
    "predicted_answer = processor.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "# show the image\n",
    "image.show()\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"True answer: {true_answer}\")\n",
    "print(f\"Predicted answer: {predicted_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Sample 1000 random samples from the validation set\n",
    "num_samples = 1000\n",
    "sampled_validation_data = random.sample(list(dataset_dict['validation']), num_samples)\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Run inference on each sample\n",
    "for data in tqdm(sampled_validation_data):\n",
    "    image_path = data['image_path'].replace('\\\\', '/')\n",
    "    question = data['question'] + \"Answer: \"\n",
    "    true_answer = data['answer']\n",
    "\n",
    "    # Load and process the image and question\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = processor(image, question, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Check for empty input_ids\n",
    "    if inputs['input_ids'].size(1) == 0:\n",
    "        print(f\"Skipping sample with empty input_ids for question: {question}\")\n",
    "        continue\n",
    "\n",
    "    # Generate prediction with max_new_tokens to avoid length issues\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=50)  # Limit the number of generated tokens\n",
    "    predicted_answer = processor.decode(out[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Append the result\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"true_answer\": true_answer,\n",
    "        \"predicted_answer\": predicted_answer\n",
    "    })\n",
    "\n",
    "# Save results to a CSV file\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"validation_results.csv\", index=False)\n",
    "\n",
    "print(\"Results saved to validation_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkgs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
